{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "### Architecture\n",
    "    * Relu in hidden layers and sigmoid in the output layer\n",
    "    * Number of layers and number of units in each layer can be set using `layers_dims` hyper-parameter\n",
    "    * Uses sigmoid cross entropy for loss computation\n",
    "   \n",
    "### Dependencies\n",
    "    * Numpy\n",
    "    * Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c14f5fc3-7ae4-4775-aca6-4e89462d83df"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.sc_utils import load_data, plot_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "812c485b-1558-4c42-bf28-17cf898d8049"
    }
   },
   "source": [
    "## Parameters Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "a1126408-fd54-4d9e-a169-b248b7656a36"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims):\n",
    "    '''\n",
    "    Arguments:\n",
    "    layers_dims -- a list of dimensions of each layer of our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing weights and biases of the network\n",
    "    '''\n",
    "\n",
    "    parameters = {}\n",
    "    for l in range(len(layers_dims)-1):\n",
    "        parameters['W' + str(l+1)] = np.random.randn(layers_dims[l+1], layers_dims[l]) * 0.01\n",
    "        parameters['b' + str(l+1)] = np.zeros([layers_dims[l+1], 1])\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fd3b570f-140d-4209-98cb-105a062f0cd7"
    }
   },
   "source": [
    "## Forward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "571916c8-3488-470d-a8ed-5091c197f9fe"
    }
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return np.maximum(X, 0)\n",
    "\n",
    "def softmax(X):\n",
    "    t = np.exp(X)\n",
    "    return  t / np.sum(t, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "965c3245-fdbd-4049-8905-98a0c913f525"
    }
   },
   "outputs": [],
   "source": [
    "def forward_propogation(A, parameters):\n",
    "    '''\n",
    "    Implement the forward propogation in the network\n",
    "    \n",
    "    Arguments:\n",
    "    A -- input to the network\n",
    "    parameters -- a dictionary containing weights and biases of the network\n",
    "    \n",
    "    Returns:\n",
    "    A -- Post activation value of the last layer\n",
    "    caches -- cache of all activation values, required for backpropogation \n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 # no. of layers\n",
    "    caches  = {}\n",
    "    for l in range(L):\n",
    "        W = parameters['W' + str(l+1)]\n",
    "        b = parameters['b' + str(l+1)]\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = relu(Z) if l<L-1 else softmax(Z) # relu in hidden layers and sigmoid in output layer\n",
    "        caches['A' + str(l+1)] = A\n",
    "        caches['Z' + str(l+1)] = Z\n",
    "\n",
    "    return A, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c4713b4b-8f80-42e8-b0e4-658123b3a73f"
    }
   },
   "source": [
    "## Cost Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "938d86d0-a5a7-4a5f-9ef8-8e349fb78f04"
    }
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function for the network\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to the label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector, shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = -1 / m * np.sum(Y * np.log(AL))\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4c54d363-8bad-4c18-8882-3b1ed3c4b7f1"
    }
   },
   "source": [
    "## Backward Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "277f8459-2d5a-4171-918b-1125ec7d553d"
    }
   },
   "outputs": [],
   "source": [
    "def softmax_backward(grad_A, A):\n",
    "    return grad_A * A * (1 - A)\n",
    "\n",
    "def relu_backward(grad_A, Z):\n",
    "    grad_A[Z<=0] = 0\n",
    "    return grad_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "9d525583-9a97-44aa-940d-94b28dd39f29"
    }
   },
   "outputs": [],
   "source": [
    "def backward_propogation(X, Y, AL, caches, parameters):\n",
    "    '''\n",
    "    Implement Backpropogation\n",
    "    \n",
    "    Arguments:\n",
    "    Al -- Activations of last layer\n",
    "    Y -- True labels of data\n",
    "    caches -- dictionary containing values of A and Z of each layer\n",
    "    parameters -- dictionary containing parameters of the network\n",
    "    \n",
    "    Returns\n",
    "    grads -- dictionary containing gradients of the network parameters\n",
    "    '''\n",
    "    grads = {}\n",
    "    m = Y.shape[1]\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    grad_A = 1/m * Y/AL\n",
    "    grad_Z =  1/m * (AL - Y)\n",
    "    \n",
    "    for l in reversed(range(1, L)):\n",
    "        grads['W' + str(l+1)] = np.dot(grad_Z, caches['A' + str(l)].T)\n",
    "        grads['b' + str(l+1)] = np.sum(grad_Z, axis=1, keepdims=True)\n",
    "        \n",
    "        assert(grads['W' + str(l+1)].shape == parameters['W' + str(l+1)].shape)\n",
    "        assert(grads['b' + str(l+1)].shape == parameters['b' + str(l+1)].shape)\n",
    "        \n",
    "        grad_A = np.dot(parameters['W' + str(l+1)].T, grad_Z)\n",
    "        grad_Z = relu_backward(grad_A, caches['Z' + str(l)])\n",
    "\n",
    "    #for first layer\n",
    "    grads['W1'] = np.dot(grad_Z, X.T)\n",
    "    grads['b1'] = np.sum(grad_Z, axis=1, keepdims=True)\n",
    "    \n",
    "    assert(grads['W1'].shape == parameters['W1'].shape)\n",
    "    assert(grads['b1'].shape == parameters['b1'].shape)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1736f426-5cad-44ad-92bc-fc6ccc6758db"
    }
   },
   "source": [
    "## Parameters Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "ef33b53a-de6b-4fb5-9111-508c9183f35f"
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    '''\n",
    "    Update parameters of the network using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    paramters -- dictionary containing weights and biases of the network\n",
    "    grads -- dictionary containing the gradients of the parameters\n",
    "    learning_rate -- rate of gradient descent\n",
    "    \n",
    "    Returns\n",
    "    parameters -- dictionary containing updated parameters\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters)//2\n",
    "    for l in reversed(range(L-1)):\n",
    "        parameters['W'+str(l+1)] -= learning_rate * grads['W' + str(l+1)]\n",
    "        parameters['b'+str(l+1)] -= learning_rate * grads['b' + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "399cefc4-a924-41c2-994a-43902692ff76"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "237535d0-fd38-4421-baaf-592076b72e67"
    }
   },
   "outputs": [],
   "source": [
    "def Model(X, Y, X_val, Y_val, layers_dims, epochs, learning_rate):\n",
    "    \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    costs_train = []\n",
    "    costs_val = []\n",
    "    \n",
    "    for epoch in range(epochs+1):\n",
    "        AL, caches = forward_propogation(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = backward_propogation(X, Y, AL, caches, parameters)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # compute validation cost\n",
    "        AL_val, _ = forward_propogation(X_val, parameters)\n",
    "        cost_val = compute_cost(AL_val, Y_val)\n",
    "        costs_train.append(cost)\n",
    "        costs_val.append(cost_val)\n",
    "        if epoch%10 == 0:\n",
    "            print('Epoch:', epoch, 'Cost: %0.3f' % cost, '- Val Cost: %0.3f' % cost_val)\n",
    "    \n",
    "    plot_training(costs_train, costs_val)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b12fa819-ef1c-468d-ac07-184adeb85519"
    }
   },
   "source": [
    "## Implementing the model on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "5457c250-fd02-466b-8286-02b5749b757b"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50000) (10, 50000) (784, 10000) (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Load MNIST dataset for 0 and 1 digits only. Find the code in utils.py\n",
    "'''\n",
    "train, val = load_data()\n",
    "X_train, Y_train = train\n",
    "X_val, Y_val = val\n",
    "\n",
    "print(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "c551d248-6228-48ec-83cf-d6ec46639245"
    }
   },
   "outputs": [],
   "source": [
    "# hyper-parameters, let's take these values for our example!\n",
    "epochs = 200\n",
    "learning_rate = 0.5\n",
    "layers_dims = [784, 512, 128, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b737c4a1-2309-4dbf-9897-4b564bb2497f"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 2.303 - Val Cost: 2.303\n",
      "Epoch: 10 Cost: 2.301 - Val Cost: 2.301\n",
      "Epoch: 20 Cost: 2.299 - Val Cost: 2.299\n",
      "Epoch: 30 Cost: 2.297 - Val Cost: 2.297\n",
      "Epoch: 40 Cost: 2.294 - Val Cost: 2.293\n",
      "Epoch: 50 Cost: 2.289 - Val Cost: 2.288\n"
     ]
    }
   ],
   "source": [
    "parameters = Model(X_train, Y_train, X_val, Y_val, layers_dims, epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML]",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
